# Week 2 ‚Äì Scaling Architecture & High Availability

> **Mentor Note**: Tu·∫ßn 1 b·∫°n ƒë√£ h·ªçc fundamentals. Tu·∫ßn 2 n√†y b·∫°n ph·∫£i TH·ª∞C H√ÄNH scaling v√† high availability. Kh√¥ng c√≥
> l√Ω thuy·∫øt su√¥ng. M·ªói concept ph·∫£i ƒë∆∞·ª£c implement v√† test. N·∫øu b·∫°n kh√¥ng code, b·∫°n kh√¥ng h·ªçc ƒë∆∞·ª£c g√¨.

---

## Study TODOs

### Horizontal vs Vertical Scaling Deep Dive

- [ ] ƒê·ªçc "Designing Data-Intensive Applications" Chapter 1 - Scaling approaches (pages 30-50)
- [ ] Vi·∫øt comparison table: Horizontal vs Vertical (10 ƒëi·ªÉm so s√°nh)
    - Compare: Concept, Scalability Limit, Cost, Availability, Fault Tolerance, Performance, Complexity, Maintenance,
      Flexibility, Use Cases
        - Concept: Vertical scaling l√† tƒÉng t√†i nguy√™n c·ªßa m·ªôt server (CPU, RAM), horizontal scaling l√† th√™m nhi·ªÅu
          server h∆°n.
        - Scalability Limit: Vertical scaling b·ªã gi·ªõi h·∫°n b·ªüi k√≠ch th∆∞·ªõc t·ªëi ƒëa c·ªßa server, horizontal scaling g·∫ßn nh∆∞
          kh√¥ng gi·ªõi h·∫°n.
        - Cost: Vertical scaling c√≥ th·ªÉ r·∫ª h∆°n ban ƒë·∫ßu nh∆∞ng ƒë·∫Øt h∆°n ·ªü quy m√¥ l·ªõn, horizontal scaling c√≥ chi ph√≠ linh
          ho·∫°t h∆°n.
        - Availability: Horizontal scaling cung c·∫•p ƒë·ªô s·∫µn s√†ng cao h∆°n do kh√¥ng c√≥ SPOF.
        - Fault Tolerance: Horizontal scaling t·ªët h∆°n trong vi·ªác ch·ªãu l·ªói v√¨ c√≥ nhi·ªÅu b·∫£n sao.
        - Performance: Vertical scaling c√≥ th·ªÉ cung c·∫•p hi·ªáu su·∫•t cao h∆°n cho c√°c t√°c v·ª• ƒë∆°n lu·ªìng.
        - Complexity: Horizontal scaling ph·ª©c t·∫°p h∆°n trong qu·∫£n l√Ω v√† ƒëi·ªÅu ph·ªëi.
        - Maintenance: Vertical scaling d·ªÖ b·∫£o tr√¨ h∆°n v√¨ √≠t th√†nh ph·∫ßn h∆°n.
        - Flexibility: Horizontal scaling linh ho·∫°t h∆°n trong vi·ªác m·ªü r·ªông v√† thu nh·ªè.
        - Use Cases: Vertical scaling ph√π h·ª£p cho c√°c ·ª©ng d·ª•ng nh·ªè, horizontal scaling cho c√°c ·ª©ng d·ª•ng l·ªõn, ph√¢n t√°n.
- [ ] Li·ªát k√™ 5 limitations c·ªßa vertical scaling
    1. Gi·ªõi h·∫°n ph·∫ßn c·ª©ng: C√≥ gi·ªõi h·∫°n v·ªÅ k√≠ch th∆∞·ªõc v√† kh·∫£ nƒÉng c·ªßa m·ªôt m√°y ch·ªß ƒë∆°n.
    2. Chi ph√≠ tƒÉng cao: M√°y ch·ªß l·ªõn h∆°n th∆∞·ªùng ƒë·∫Øt ƒë·ªè h∆°n v√† chi ph√≠ b·∫£o tr√¨ c≈©ng cao h∆°n.
    3. Single Point of Failure (SPOF): N·∫øu m√°y ch·ªß g·∫∑p s·ª± c·ªë, to√†n b·ªô h·ªá th·ªëng c√≥ th·ªÉ b·ªã ·∫£nh h∆∞·ªüng.
    4. Downtime during upgrades: N√¢ng c·∫•p ph·∫ßn c·ª©ng c√≥ th·ªÉ y√™u c·∫ßu downtime.
    5. Kh·∫£ nƒÉng m·ªü r·ªông h·∫°n ch·∫ø: Kh√¥ng th·ªÉ m·ªü r·ªông v√¥ h·∫°n ch·ªâ b·∫±ng c√°ch n√¢ng c·∫•p m·ªôt m√°y ch·ªß.
- [ ] Li·ªát k√™ 5 challenges c·ªßa horizontal scaling
    1. Ph·ª©c t·∫°p trong qu·∫£n l√Ω: C·∫ßn c√≥ h·ªá th·ªëng ƒë·ªÉ qu·∫£n l√Ω nhi·ªÅu m√°y ch·ªß.
    2. ƒê·ªìng b·ªô d·ªØ li·ªáu: C·∫ßn gi·∫£i ph√°p ƒë·ªÉ ƒë·ªìng b·ªô d·ªØ li·ªáu gi·ªØa c√°c m√°y ch·ªß.
    3. Chi ph√≠ m·∫°ng: Giao ti·∫øp gi·ªØa c√°c m√°y ch·ªß c√≥ th·ªÉ t·∫°o ra chi ph√≠ m·∫°ng ƒë√°ng k·ªÉ.
    4. Load balancing: C·∫ßn c√≥ c∆° ch·∫ø ph√¢n ph·ªëi t·∫£i hi·ªáu qu·∫£.
    5. Gi√°m s√°t v√† b·∫£o tr√¨: C·∫ßn h·ªá th·ªëng gi√°m s√°t ƒë·ªÉ theo d√µi tr·∫°ng th√°i c·ªßa nhi·ªÅu m√°y ch·ªß.

- [ ] ƒê·ªçc v·ªÅ "scaling laws" - khi n√†o vertical scaling fails?
    - Vertical scaling th∆∞·ªùng th·∫•t b·∫°i khi:
        1. ƒê·∫°t ƒë·∫øn gi·ªõi h·∫°n ph·∫ßn c·ª©ng t·ªëi ƒëa c·ªßa m√°y ch·ªß.
        2. Chi ph√≠ n√¢ng c·∫•p v∆∞·ª£t qu√° l·ª£i √≠ch thu ƒë∆∞·ª£c.
        3. Y√™u c·∫ßu v·ªÅ ƒë·ªô s·∫µn s√†ng v√† ch·ªãu l·ªói cao kh√¥ng th·ªÉ ƒë√°p ·ª©ng.
        4. TƒÉng t·∫£i ƒë·ªôt ng·ªôt v∆∞·ª£t qu√° kh·∫£ nƒÉng x·ª≠ l√Ω c·ªßa m√°y ch·ªß ƒë∆°n.
        5. C·∫ßn m·ªü r·ªông linh ho·∫°t v√† nhanh ch√≥ng m√† vertical scaling kh√¥ng th·ªÉ ƒë√°p ·ª©ng k·ªãp th·ªùi.
- [ ] Research: Maximum server size available (CPU cores, RAM) - current limits
    - Hi·ªán t·∫°i, c√°c m√°y ch·ªß th∆∞∆°ng m·∫°i c√≥ th·ªÉ c√≥ t·ªõi 128 CPU cores v√† 4TB RAM, nh∆∞ng c√°c m√°y ch·ªß chuy√™n d·ª•ng c√≥ th·ªÉ v∆∞·ª£t
      qua con s·ªë n√†y.
- [ ] Research: Cost comparison - 1 large server vs 10 small servers (same capacity)
    - Chi ph√≠ c·ªßa m·ªôt m√°y ch·ªß l·ªõn c√≥ th·ªÉ cao h∆°n 20-30% so v·ªõi vi·ªác s·ª≠ d·ª•ng 10 m√°y ch·ªß nh·ªè h∆°n v·ªõi t·ªïng c√¥ng su·∫•t t∆∞∆°ng
      ƒë∆∞∆°ng, nh∆∞ng chi ph√≠ b·∫£o tr√¨ v√† qu·∫£n l√Ω c√≥ th·ªÉ th·∫•p h∆°n.
    - Tuy nhi√™n, chi ph√≠ m·∫°ng v√† ph·ª©c t·∫°p trong qu·∫£n l√Ω c√≥ th·ªÉ l√†m tƒÉng t·ªïng chi ph√≠ s·ªü h·ªØu (TCO) c·ªßa gi·∫£i ph√°p
      horizontal scaling.
    - T·ªïng chi ph√≠ ph·ª• thu·ªôc v√†o nhi·ªÅu y·∫øu t·ªë nh∆∞ nh√† cung c·∫•p, c·∫•u h√¨nh c·ª• th·ªÉ v√† y√™u c·∫ßu v·ªÅ hi·ªáu su·∫•t.
    - N√™n th·ª±c hi·ªán ph√¢n t√≠ch chi ph√≠ c·ª• th·ªÉ d·ª±a tr√™n nhu c·∫ßu v√† m√¥i tr∆∞·ªùng tri·ªÉn khai.
    - T√≥m l·∫°i, chi ph√≠ ban ƒë·∫ßu c·ªßa vertical scaling c√≥ th·ªÉ th·∫•p h∆°n, nh∆∞ng horizontal scaling cung c·∫•p s·ª± linh ho·∫°t v√†
      ƒë·ªô s·∫µn s√†ng cao h∆°n v·ªõi chi ph√≠ d√†i h·∫°n c√≥ th·ªÉ c·∫°nh tranh.
- [ ] ƒê·ªçc v·ªÅ "scaling out" vs "scaling up" terminology
    - Scaling out (horizontal scaling) l√† qu√° tr√¨nh th√™m nhi·ªÅu m√°y ch·ªß ho·∫∑c n√∫t v√†o h·ªá th·ªëng ƒë·ªÉ tƒÉng kh·∫£ nƒÉng x·ª≠ l√Ω.
    - Scaling up (vertical scaling) l√† qu√° tr√¨nh n√¢ng c·∫•p t√†i nguy√™n c·ªßa m·ªôt m√°y ch·ªß hi·ªán c√≥, nh∆∞ tƒÉng CPU, RAM ho·∫∑c
      dung l∆∞·ª£ng l∆∞u tr·ªØ.
- [ ] T√¨m 3 systems that MUST use horizontal scaling (v√† t·∫°i sao)
    1. Google Search: C·∫ßn x·ª≠ l√Ω h√†ng t·ª∑ truy v·∫•n m·ªói ng√†y, y√™u c·∫ßu kh·∫£ nƒÉng m·ªü r·ªông linh ho·∫°t v√† ƒë·ªô s·∫µn s√†ng cao.
    2. Facebook: V·ªõi h√†ng t·ª∑ ng∆∞·ªùi d√πng v√† l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì, horizontal scaling gi√∫p duy tr√¨ hi·ªáu su·∫•t v√† ƒë·ªô s·∫µn
       s√†ng.
    3. Amazon Web Services (AWS): Cung c·∫•p d·ªãch v·ª• ƒë√°m m√¢y cho h√†ng tri·ªáu kh√°ch h√†ng, y√™u c·∫ßu kh·∫£ nƒÉng m·ªü r·ªông nhanh
       ch√≥ng v√† linh ho·∫°t ƒë·ªÉ ƒë√°p ·ª©ng nhu c·∫ßu thay ƒë·ªïi.
- [ ] T√¨m 2 systems that CAN use vertical scaling (v√† t·∫°i sao h·ªç ch·ªçn)
    1. Small Business Websites: C√°c trang web nh·ªè th∆∞·ªùng c√≥ l∆∞u l∆∞·ª£ng truy c·∫≠p th·∫•p v√† kh√¥ng y√™u c·∫ßu kh·∫£ nƒÉng m·ªü r·ªông
       l·ªõn, do ƒë√≥ vertical scaling l√† ƒë·ªß v√† ti·∫øt ki·ªám chi ph√≠.
    2. Legacy Enterprise Applications: Nhi·ªÅu ·ª©ng d·ª•ng doanh nghi·ªáp c≈© ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ch·∫°y tr√™n m·ªôt m√°y ch·ªß duy nh·∫•t v√†
       c√≥ th·ªÉ kh√¥ng c·∫ßn m·ªü r·ªông l·ªõn, do ƒë√≥ vertical scaling l√† l·ª±a ch·ªçn h·ª£p l√Ω ƒë·ªÉ duy tr√¨ hi·ªáu su·∫•t m√† kh√¥ng c·∫ßn thay
       ƒë·ªïi ki·∫øn tr√∫c.

### Load Balancer Strategies

- [ ] ƒê·ªçc v·ªÅ 5 load balancing algorithms: Round-robin, Weighted Round-robin, Least Connections, IP Hash, Least Response
  Time
    1. Round-robin: Ph√¢n ph·ªëi c√°c y√™u c·∫ßu ƒë·∫øn c√°c m√°y ch·ªß theo th·ª© t·ª± tu·∫ßn t·ª±.
    2. Weighted Round-robin: T∆∞∆°ng t·ª± nh∆∞ round-robin nh∆∞ng ph√¢n ph·ªëi d·ª±a tr√™n tr·ªçng s·ªë ƒë∆∞·ª£c g√°n cho m·ªói m√°y ch·ªß.
    3. Least Connections: G·ª≠i y√™u c·∫ßu ƒë·∫øn m√°y ch·ªß c√≥ √≠t k·∫øt n·ªëi hi·ªán t·∫°i nh·∫•t.
    4. IP Hash: S·ª≠ d·ª•ng ƒë·ªãa ch·ªâ IP c·ªßa kh√°ch h√†ng ƒë·ªÉ x√°c ƒë·ªãnh m√°y ch·ªß nh·∫≠n y√™u c·∫ßu, gi√∫p duy tr√¨ phi√™n l√†m vi·ªác (sticky
       sessions).
    5. Least Response Time: G·ª≠i y√™u c·∫ßu ƒë·∫øn m√°y ch·ªß c√≥ th·ªùi gian ph·∫£n h·ªìi nhanh nh·∫•t.

- [ ] Vi·∫øt algorithm pseudocode cho m·ªói algorithm
    - Round-robin:
        ```
        currentIndex = 0
        function getNextServer(servers):
            server = servers[currentIndex]
            currentIndex = (currentIndex + 1) % length(servers)
            return server
        ```
    - Weighted Round-robin:
        ```
        weights = [weight1, weight2, ..., weightN]
        currentIndex = -1
        currentWeight = 0
        function getNextServer(servers):
            while true:
                currentIndex = (currentIndex + 1) % length(servers)
                if currentIndex == 0:
                    currentWeight = currentWeight - gcd(weights)
                    if currentWeight <= 0:
                        currentWeight = max(weights)
                        if currentWeight == 0:
                            return null
                if weights[currentIndex] >= currentWeight:
                    return servers[currentIndex]
        ```
    - Least Connections:
        ```
        function getNextServer(servers):
            minConnections = infinity
            selectedServer = null
            for server in servers:
                if server.activeConnections < minConnections:
                    minConnections = server.activeConnections
                    selectedServer = server
            return selectedServer
        ```
    - IP Hash:
        ```
        function getNextServer(servers, clientIP):
            hashValue = hash(clientIP)
            index = hashValue % length(servers)
            return servers[index]
        ```
    - Least Response Time:
        ```
        function getNextServer(servers):
            minResponseTime = infinity
            selectedServer = null
            for server in servers:
                if server.responseTime < minResponseTime:
                    minResponseTime = server.responseTime
                    selectedServer = server
            return selectedServer
        ```
- [ ] So s√°nh: Round-robin vs Least Connections (3 use cases m·ªói c√°i)
    - Round-robin:
        1. Use Case 1: ·ª®ng d·ª•ng web v·ªõi t·∫£i ƒë·ªìng ƒë·ªÅu, n∆°i m·ªói m√°y ch·ªß c√≥ kh·∫£ nƒÉng x·ª≠ l√Ω t∆∞∆°ng t·ª±.
        2. Use Case 2: H·ªá th·ªëng kh√¥ng y√™u c·∫ßu duy tr√¨ phi√™n l√†m vi·ªác (stateless applications).
        3. Use Case 3: M√¥i tr∆∞·ªùng th·ª≠ nghi·ªám ho·∫∑c ph√°t tri·ªÉn v·ªõi s·ªë l∆∞·ª£ng m√°y ch·ªß nh·ªè.
    - Least Connections:
        1. Use Case 1: ·ª®ng d·ª•ng web v·ªõi t·∫£i kh√¥ng ƒë·ªìng ƒë·ªÅu, n∆°i m·ªôt s·ªë m√°y ch·ªß c√≥ th·ªÉ x·ª≠ l√Ω nhi·ªÅu k·∫øt n·ªëi h∆°n.
        2. Use Case 2: H·ªá th·ªëng c√≥ c√°c phi√™n l√†m vi·ªác d√†i, n∆°i m·ªôt s·ªë k·∫øt n·ªëi c√≥ th·ªÉ chi·∫øm nhi·ªÅu t√†i nguy√™n h∆°n.
        3. Use Case 3: ·ª®ng d·ª•ng th·ªùi gian th·ª±c nh∆∞ chat ho·∫∑c streaming, n∆°i k·∫øt n·ªëi c√≥ th·ªÉ k√©o d√†i.
- [ ] ƒê·ªçc v·ªÅ "sticky sessions" (session affinity)
    - Sticky sessions l√† m·ªôt k·ªπ thu·∫≠t trong load balancing ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng c√°c y√™u c·∫ßu t·ª´ c√πng m·ªôt ng∆∞·ªùi d√πng (ho·∫∑c
      phi√™n l√†m vi·ªác) lu√¥n ƒë∆∞·ª£c g·ª≠i ƒë·∫øn c√πng m·ªôt m√°y ch·ªß backend. ƒêi·ªÅu n√†y quan tr·ªçng ƒë·ªëi v·ªõi c√°c ·ª©ng d·ª•ng stateful, n∆°i
      tr·∫°ng th√°i c·ªßa ng∆∞·ªùi d√πng ƒë∆∞·ª£c l∆∞u tr·ªØ tr√™n m√°y ch·ªß c·ª• th·ªÉ
- [ ] Analyze: Khi n√†o c·∫ßn sticky sessions? Trade-offs?
    - C·∫ßn sticky sessions khi:
        1. ·ª®ng d·ª•ng stateful: Khi tr·∫°ng th√°i ng∆∞·ªùi d√πng ƒë∆∞·ª£c l∆∞u tr·ªØ tr√™n m√°y ch·ªß c·ª• th·ªÉ.
        2. Phi√™n l√†m vi·ªác d√†i: Khi ng∆∞·ªùi d√πng c√≥ c√°c t∆∞∆°ng t√°c k√©o d√†i v·ªõi ·ª©ng d·ª•ng.
        3. Y√™u c·∫ßu hi·ªáu su·∫•t: Khi vi·ªác duy tr√¨ phi√™n l√†m vi·ªác tr√™n c√πng m·ªôt m√°y ch·ªß gi√∫p gi·∫£m ƒë·ªô tr·ªÖ.
- [ ] ƒê·ªçc v·ªÅ Layer 4 (L4) vs Layer 7 (L7) load balancing
    - Layer 4 (L4) load balancing ho·∫°t ƒë·ªông ·ªü t·∫ßng v·∫≠n chuy·ªÉn c·ªßa m√¥ h√¨nh OSI, s·ª≠ d·ª•ng th√¥ng tin nh∆∞ ƒë·ªãa ch·ªâ IP v√† c·ªïng
      ƒë·ªÉ ph√¢n ph·ªëi l∆∞u l∆∞·ª£ng.
    - Layer 7 (L7) load balancing ho·∫°t ƒë·ªông ·ªü t·∫ßng ·ª©ng d·ª•ng, s·ª≠ d·ª•ng th√¥ng tin chi ti·∫øt h∆°n nh∆∞ URL, ti√™u ƒë·ªÅ HTTP ƒë·ªÉ ƒë∆∞a
      ra quy·∫øt ƒë·ªãnh ph√¢n ph·ªëi.
    - L4 load balancing th∆∞·ªùng nhanh h∆°n v√† √≠t ph·ª©c t·∫°p h∆°n
    - Trong khi L7 load balancing cung c·∫•p kh·∫£ nƒÉng t√πy ch·ªânh cao h∆°n v√† h·ªó tr·ª£ c√°c t√≠nh nƒÉng nh∆∞ SSL termination,
      cookie-based routing.
    - L4 load balancing ph√π h·ª£p cho c√°c ·ª©ng d·ª•ng ƒë∆°n gi·∫£n v√† y√™u c·∫ßu hi·ªáu su·∫•t cao
    - Trong khi L7 load balancing th√≠ch h·ª£p cho c√°c ·ª©ng d·ª•ng ph·ª©c t·∫°p c·∫ßn x·ª≠ l√Ω logic ·ª©ng d·ª•ng.
- [ ] So s√°nh: L4 vs L7 (performance, features, use cases)
    - Performance: L4 load balancing nhanh h∆°n do x·ª≠ l√Ω √≠t th√¥ng tin h∆°n, trong khi L7 load balancing ch·∫≠m h∆°n do ph√¢n
      t√≠ch s√¢u h∆°n.
    - Features: L7 load balancing cung c·∫•p nhi·ªÅu t√≠nh nƒÉng h∆°n nh∆∞ SSL termination, URL-based routing, trong khi L4 t·∫≠p
      trung v√†o ph√¢n ph·ªëi c∆° b·∫£n.
    - Use Cases: L4 ph√π h·ª£p cho c√°c ·ª©ng d·ª•ng ƒë∆°n gi·∫£n v√† y√™u c·∫ßu hi·ªáu su·∫•t cao, trong khi L7 th√≠ch h·ª£p cho c√°c ·ª©ng d·ª•ng
      ph·ª©c t·∫°p c·∫ßn x·ª≠ l√Ω logic ·ª©ng d·ª•ng.
- [ ] Research: Popular load balancers (Nginx, HAProxy, AWS ELB, F5)
    - Nginx: M·ªôt web server v√† reverse proxy ph·ªï bi·∫øn, h·ªó tr·ª£ L7 load balancing v·ªõi nhi·ªÅu t√≠nh nƒÉng nh∆∞ SSL termination,
      caching.
    - V√≠ d·ª• s·ª≠ d·ª•ng: Netflix, Airbnb.
    - HAProxy: M·ªôt load balancer hi·ªáu su·∫•t cao, h·ªó tr·ª£ c·∫£ L4 v√† L7 load balancing, th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng trong c√°c m√¥i
      tr∆∞·ªùng y√™u c·∫ßu hi·ªáu su·∫•t cao.
    - AWS ELB (Elastic Load Balancing): D·ªãch v·ª• load balancing c·ªßa Amazon Web Services, h·ªó tr·ª£ c·∫£ L4 v√† L7, t√≠ch h·ª£p t·ªët
      v·ªõi c√°c d·ªãch v·ª• AWS kh√°c.
    - F5: M·ªôt gi·∫£i ph√°p load balancing doanh nghi·ªáp v·ªõi nhi·ªÅu t√≠nh nƒÉng n√¢ng cao nh∆∞ b·∫£o m·∫≠t ·ª©ng d·ª•ng, t·ªëi ∆∞u h√≥a hi·ªáu
      su·∫•t.
      Ok. M√¨nh vi·∫øt **ng·∫Øn ‚Äì ƒë√∫ng th·ª±c t·∫ø ‚Äì kh√¥ng lan man**.

---

# üìò ·ª®NG D·ª§NG TH·ª∞C T·∫æ C·ª¶A NGINX ‚Äì HAPROXY ‚Äì AWS ELB ‚Äì F5 (VI·∫æT CHU·∫®N TH·ª∞C CHI·∫æN)

M·ª•c ti√™u: hi·ªÉu ƒë√∫ng **vai tr√≤**, **ƒëi·ªÉm m·∫°nh**, **v·ªã tr√≠ trong ki·∫øn tr√∫c**, v√† **khi n√†o ch·ªçn c√°i n√†o**.

---

## 0. M·ªôt c√¢u ƒë·ªÉ nh·ªõ

- **NGINX**: reverse proxy/app edge ‚Äúg·∫ßn ·ª©ng d·ª•ng‚Äù (HTTP focus).
- **HAProxy**: load balancer ‚Äúc·ª©ng‚Äù ·ªü **L4/TCP**, c≈©ng l√†m ƒë∆∞·ª£c L7.
- **AWS ELB**: load balancer **managed** tr√™n AWS (ALB/NLB/GWLB).
- **F5**: **ADC enterprise** + policy + security modules (WAF/DDoS/TLS‚Ä¶ tu·ª≥ gi·∫£i ph√°p).

---

## 1. NGINX ‚Äì REVERSE PROXY / APP EDGE (L7)

### D√πng ƒë·ªÉ l√†m g√¨ ngo√†i ƒë·ªùi (ph·ªï bi·∫øn nh·∫•t):

- Reverse proxy cho backend (hide origin, unify routing)
- TLS termination (SSL offload)
- Routing/rewrite/redirect
- Basic controls: allow/deny, rate limit, header controls
- Caching (static + m·ªôt ph·∫ßn dynamic khi h·ª£p l√Ω)
- Serve static (assets)

### V·ªã tr√≠ ƒëi·ªÉn h√¨nh:

```
Client ‚Üí (CDN/WAF/LB tu·ª≥) ‚Üí Nginx ‚Üí App
```

### Khi n√†o d√πng (rule-of-thumb):

- B·∫°n c·∫ßn **gateway g·∫ßn app** ƒë·ªÉ chu·∫©n ho√°: TLS, routing, rate limit, cache, header‚Ä¶ v√† mu·ªën t·ª± v·∫≠n h√†nh c·∫•u h√¨nh.
- N·∫øu d√πng cloud-managed gateway/proxy (ALB/API Gateway/Envoy/Istio ingress), **c√≥ th·ªÉ kh√¥ng c·∫ßn Nginx ri√™ng**.

---

## 2. HAPROXY ‚Äì LOAD BALANCER M·∫†NH ·ªû L4 (TCP) (V√Ä C≈®NG L√ÄM ƒê∆Ø·ª¢C L7)

### D√πng ƒë·ªÉ l√†m g√¨ ngo√†i ƒë·ªùi:

- L4/TCP load balancing (hi·ªáu nƒÉng cao, ·ªïn ƒë·ªãnh cho connection d√†i)
- Health check t·ªët, routing/failover theo t√¨nh tr·∫°ng backend
- WebSocket/long-lived connections (tu·ª≥ design; Nginx c≈©ng l√†m ƒë∆∞·ª£c)
- Internal LB (on-prem/self-host) cho service/cluster
- Routing cho DB layer theo vai tr√≤ (primary/replica) **n·∫øu b·∫°n ƒë√£ c√≥ c∆° ch·∫ø x√°c ƒë·ªãnh vai tr√≤**

### ƒêi·ªÉm d·ªÖ hi·ªÉu sai (quan tr·ªçng):

> HAProxy **kh√¥ng t·ª± b·∫ßu ch·ªçn/ƒë·ªÅ c·ª≠ master DB**. N√≥ ch·ªâ **route traffic** theo health-check.  
> Promote/failover DB l√† vi·ªác c·ªßa DB/tooling (v√≠ d·ª• Postgres: Patroni; MySQL: Orchestrator; Redis: Sentinel‚Ä¶).

### V·ªã tr√≠ ƒëi·ªÉn h√¨nh:

```
Client/App ‚Üí HAProxy ‚Üí Backend nodes (TCP/HTTP)
```

### Khi n√†o d√πng (rule-of-thumb):

- Khi workload thi√™n v·ªÅ **TCP/L4**, connection d√†i, c·∫ßn LB ‚Äúth·ª±c d·ª•ng‚Äù, observability/health-check r√µ r√†ng.
- Khi b·∫°n self-host v√† mu·ªën **LB trung t√¢m** cho nhi·ªÅu cluster (DB, MQ, realtime).

---

## 3. AWS ELB ‚Äì LOAD BALANCER MANAGED TR√äN AWS

### Ch·ªçn ƒë√∫ng ‚Äúlo·∫°i‚Äù:

- **ALB**: L7 HTTP/HTTPS, host/path routing, ph√π h·ª£p web/app API.
- **NLB**: L4 TCP/UDP/TLS, hi·ªáu nƒÉng cao, IP-based, ph√π h·ª£p TCP/realtime.
- **GWLB**: ‚Äúch√®n‚Äù security appliances (appliance insertion) v√†o traffic path.

### D√πng ƒë·ªÉ l√†m g√¨ ngo√†i ƒë·ªùi:

- Multi-AZ high availability
- Target groups + health checks
- TLS termination (ALB/NLB TLS)
- T√≠ch h·ª£p ECS/EKS/EC2/Auto Scaling
- H·ªó tr·ª£ blue/green/canary th√¥ng qua target groups/weights (k·∫øt h·ª£p deploy tooling tu·ª≥ c√°ch l√†m)

### V·ªã tr√≠ ƒëi·ªÉn h√¨nh:

```
Internet ‚Üí ALB/NLB/GWLB ‚Üí Targets (EC2/ECS/EKS/Pods)
```

### Khi n√†o d√πng (rule-of-thumb):

- Tr√™n AWS th√¨ **∆∞u ti√™n ELB tr∆∞·ªõc**, v√¨ n√≥ managed, HA s·∫µn, t√≠ch h·ª£p h·ªá sinh th√°i t·ªët.
- Ch·ªâ th√™m Nginx/Envoy ‚Äúg·∫ßn app‚Äù khi b·∫°n c·∫ßn: caching, rewrite ph·ª©c t·∫°p, custom behaviors, ho·∫∑c standardize gi·ªëng on-prem.

---

## 4. F5 ‚Äì ADC ENTERPRISE + POLICY + SECURITY (TU·ª≤ MODULE)

### D√πng ƒë·ªÉ l√†m g√¨ ngo√†i ƒë·ªùi (tu·ª≥ c√¥ng ty/mua license):

- ADC/traffic management enterprise (LB, routing policy, TLS policy)
- WAF (v√≠ d·ª• BIG-IP ASM/Advanced WAF)
- DDoS/Firewall capabilities (tu·ª≥ module/gi·∫£i ph√°p)
- TLS offload quy m√¥ l·ªõn + ki·ªÉm so√°t cipher/policy
- T√≠ch h·ª£p SSO/identity/policy (tu·ª≥ ki·∫øn tr√∫c)

### V·ªã tr√≠ ƒëi·ªÉn h√¨nh:

```
Internet ‚Üí (F5/WAF/ADC) ‚Üí (LB) ‚Üí App/Core systems
```

### Khi n√†o d√πng (rule-of-thumb):

- Enterprise/on-prem/hybrid, y√™u c·∫ßu policy, audit, quy tr√¨nh thay ƒë·ªïi ch·∫∑t, v√† c·∫ßn gi·∫£i ph√°p ‚Äúƒë√≥ng g√≥i‚Äù cho traffic + security.
- Kh√¥ng ph·∫£i ‚Äúng√¢n h√†ng m·ªõi d√πng‚Äù; nh∆∞ng trong regulated/enterprise th√¨ **t·ª∑ l·ªá g·∫∑p cao**.

---

## 5. KI·∫æN TR√öC TH·ª∞C T·∫æ PH·ªî BI·∫æN (M·∫™U NHANH)

### A) Startup / SME (ƒë∆°n gi·∫£n, t·ª± host)

```
Nginx ‚Üí App
```

### B) Web/SaaS tr√™n AWS (managed-first)

```
CDN/WAF ‚Üí ALB ‚Üí App (ECS/EKS/EC2)
```

### C) AWS + c·∫ßn ‚Äúapp edge‚Äù ri√™ng (cache/rewrite/chu·∫©n ho√°)

```
CDN/WAF ‚Üí ALB ‚Üí Nginx/Envoy ‚Üí App
```

### D) DB/cluster/realtime TCP self-host

```
HAProxy ‚Üí TCP/Cluster nodes
```

### E) Enterprise on-prem/hybrid (policy + security n·∫∑ng)

```
WAF/ADC (F5) ‚Üí LB/Proxy ‚Üí App/Core
```

---

## 6. SO S√ÅNH NHANH (ƒê√öNG TR·ªåNG T√ÇM)

| Tool    | Th·∫ø m·∫°nh ch√≠nh | Layer hay d√πng | Khi ch·ªçn nhanh |
|---------|-----------------|----------------|----------------|
| Nginx   | Reverse proxy g·∫ßn app, TLS, cache, routing | L7 | Web/API edge c·∫ßn t·ª± control |
| HAProxy | LB L4 ‚Äúc·ª©ng‚Äù, health-check r√µ | L4 (c≈©ng L7) | TCP/cluster/realtime/self-host |
| ELB     | Managed HA + t√≠ch h·ª£p AWS | ALB(L7)/NLB(L4)/GWLB | Ch·∫°y tr√™n AWS |
| F5      | ADC enterprise + policy + security modules | L7/L4 tu·ª≥ | Enterprise/regulated/on-prem/hybrid |

---

## 7. QUY T·∫ÆC CH·ªåN (R·∫§T TH·ª∞C T·∫æ)

- N·∫øu b·∫°n **ƒëang ·ªü AWS**: ch·ªçn **ALB/NLB/GWLB** theo nhu c·∫ßu tr∆∞·ªõc.
- N·∫øu b·∫°n c·∫ßn **app edge t·ª± ki·ªÉm so√°t** (TLS/routing/cache/rate limit): th√™m **Nginx/Envoy**.
- N·∫øu b·∫°n c·∫ßn **L4/TCP LB** ho·∫∑c self-host cluster: **HAProxy** l√† l·ª±a ch·ªçn r·∫•t m·∫°nh.
- N·∫øu b·∫°n ·ªü enterprise v√† ‚Äúsecurity/policy/compliance-driven‚Äù: **F5/ADC/WAF enterprise** th∆∞·ªùng xu·∫•t hi·ªán.

Ghi ch√∫: Kh√¥ng c√≥ ‚Äúone size fits all‚Äù. Ch·ªçn theo **L4 vs L7**, **managed vs self-host**, **security/policy**, v√† **operational cost**.

- [ ] Compare: Nginx vs HAProxy (features, performance, use cases)
- [ ] ƒê·ªçc v·ªÅ "health checks" trong load balancers
- [ ] ƒê·ªçc v·ªÅ "graceful degradation" khi backend fails

### Stateless vs Stateful Services

- [ ] ƒê·ªãnh nghƒ©a ch√≠nh x√°c: Stateless service
- **Stateless service**: service m√† **m·ªói request t·ª± ch·ª©a ƒë·ªß context** ƒë·ªÉ x·ª≠ l√Ω (auth/context n·∫±m trong request token/headers), v√† **kh√¥ng ph·ª• thu·ªôc state c·ª•c b·ªô tr√™n instance** gi·ªØa c√°c request.
  - State v·∫´n t·ªìn t·∫°i, nh∆∞ng ƒë∆∞·ª£c **externalize** (DB/Redis/object storage/queue) thay v√¨ n·∫±m trong memory/disk c·ª•c b·ªô c·ªßa instance.
  - H·ªá qu·∫£: c√≥ th·ªÉ scale horizontal d·ªÖ, thay instance kh√¥ng l√†m ‚Äúm·∫•t session‚Äù.
- [ ] ƒê·ªãnh nghƒ©a ch√≠nh x√°c: Stateful service
- **Stateful service**: service m√† x·ª≠ l√Ω request **ph·ª• thu·ªôc state ƒë∆∞·ª£c gi·ªØ l·∫°i tr√™n instance** (in-memory session, local cache mang t√≠nh quy·∫øt ƒë·ªãnh, local files...), n√™n request sau **c·∫ßn quay l·∫°i ƒë√∫ng instance** ho·∫∑c ph·∫£i replicate state.
- [ ] Li·ªát k√™ 5 characteristics c·ªßa stateless service
    1. Kh√¥ng y√™u c·∫ßu sticky session ƒë·ªÉ ƒë√∫ng ch·ª©c nƒÉng (c√≥ th·ªÉ d√πng, nh∆∞ng kh√¥ng ‚Äúb·∫Øt bu·ªôc‚Äù).
    2. Thay/kill instance kh√¥ng l√†m m·∫•t phi√™n (kh√¥ng m·∫•t ‚Äúuser state‚Äù t·∫°i app layer).
    3. Scale in/out nhanh, ph√π h·ª£p autoscaling.
    4. D·ªÖ rolling/canary v√¨ instance interchangeable.
    5. State n·∫±m ·ªü storage chung: DB/Redis/Kafka/S3‚Ä¶ (c√≥ chi·∫øn l∆∞·ª£c consistency r√µ).
- [ ] Li·ªát k√™ 5 characteristics c·ªßa stateful service
    1. C√≥ session/state c·ª•c b·ªô (memory/disk) ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp t·ªõi response.
    2. Th∆∞·ªùng c·∫ßn sticky sessions ho·∫∑c state replication.
    3. Scale horizontal kh√≥ h∆°n (sharding theo user/room/key‚Ä¶).
    4. Deploy/failover ph·ª©c t·∫°p v√¨ ph·∫£i drain/transfer state.
    5. Risk m·∫•t d·ªØ li·ªáu phi√™n khi instance ch·∫øt (n·∫øu kh√¥ng replicate).
- [ ] Analyze: REST API - stateless hay stateful? T·∫°i sao?
    - **REST API ‚Äúƒë√∫ng chu·∫©n‚Äù l√† stateless**: server kh√¥ng gi·ªØ session state gi·ªØa c√°c request; m·ªói request mang ƒë·ªß info (token, params).
    - Th·ª±c t·∫ø: v·∫´n c√≥ th·ªÉ ‚Äúbi·∫øn t∆∞·ªõng‚Äù stateful (server-side session, in-memory cart‚Ä¶), nh∆∞ng s·∫Ω gi·∫£m kh·∫£ nƒÉng scale/HA.
- [ ] Analyze: WebSocket connection - stateless hay stateful? T·∫°i sao?
    - **WebSocket b·∫£n ch·∫•t l√† stateful ·ªü t·∫ßng connection**: c√≥ connection d√†i, state nh∆∞ subscription, room membership, in-flight messages‚Ä¶
    - C√≥ th·ªÉ ‚Äúgi·∫£m statefulness‚Äù ·ªü app b·∫±ng c√°ch:
        - Externalize presence/subscriptions v√†o Redis (pub/sub), Kafka, etc.
        - D√πng consistent hashing/sharding theo user/room ƒë·ªÉ route ·ªïn ƒë·ªãnh.
        - D√πng reconnect + resume token (idempotency) ƒë·ªÉ ch·ªãu l·ªói.
- [ ] ƒê·ªçc v·ªÅ "session state" - where to store?
    - **Options**:
        - Client-side: JWT (stateless auth) ‚Äî kh√¥ng store session server, nh∆∞ng ph·∫£i x·ª≠ l√Ω revoke/rotation.
        - Server-side external store: Redis (ph·ªï bi·∫øn), DB (n·∫∑ng), distributed cache.
        - Tr√°nh: in-memory session n·∫øu c·∫ßn scale/HA.
- [ ] Compare: In-memory session vs Redis session vs Database session (3 pros/cons m·ªói c√°i)
    - **In-memory session**
        - Pros: nhanh, ƒë∆°n gi·∫£n, r·∫ª khi 1 instance.
        - Cons: m·∫•t session khi instance ch·∫øt, c·∫ßn sticky session, kh√≥ scale.
    - **Redis session**
        - Pros: nhanh, shared gi·ªØa nhi·ªÅu instance, h·ªó tr·ª£ TTL, scale t·ªët.
        - Cons: th√™m dependency, c·∫ßn HA cho Redis, c√≥ latency m·∫°ng.
    - **Database session**
        - Pros: durable, d·ªÖ audit, consistency r√µ.
        - Cons: load cao, latency cao h∆°n, d·ªÖ th√†nh bottleneck.
- [ ] ƒê·ªçc v·ªÅ "stateless authentication" (JWT tokens)
    - JWT: client gi·ªØ token; server verify signature + claims.
    - C·∫ßn: key rotation, expiry ng·∫Øn + refresh token, ch·ªëng replay (tu·ª≥ threat model).
- [ ] ƒê·ªçc v·ªÅ "stateful authentication" (server-side sessions)
    - Session id (cookie) tr·ªè t·ªõi session store server-side (memory/Redis/DB).
    - D·ªÖ revoke ngay, nh∆∞ng c·∫ßn store + scale strategy.
- [ ] Analyze: Khi n√†o c·∫ßn stateful service? (3 use cases)
    1. Realtime apps c·∫ßn connection state: chat, trading UI, multiplayer game (WebSocket).
    2. Streaming/long-running workflows c√≥ state machine t·∫°i edge (ƒë√¥i khi).
    3. C√°c h·ªá th·ªëng c·∫ßn local state ƒë·ªÉ t·ªëi ∆∞u c·ª±c m·∫°nh v√† ch·∫•p nh·∫≠n trade-off (hi·∫øm; th∆∞·ªùng v·∫´n externalize).
- [ ] Analyze: Khi n√†o MUST use stateless? (3 use cases)
    1. Public REST APIs c·∫ßn autoscaling + rolling deploy li√™n t·ª•c.
    2. Payment/critical APIs c·∫ßn HA cao, failover nhanh, kh√¥ng ph·ª• thu·ªôc instance.
    3. Microservices ch·∫°y tr√™n Kubernetes/ASG: instance ephemeral ‚Üí stateless l√† m·∫∑c ƒë·ªãnh t·ªët.
- [ ] ƒê·ªçc v·ªÅ "shared nothing architecture"
    - ‚ÄúShared nothing‚Äù: m·ªói node kh√¥ng chia s·∫ª memory/disk; ph·ªëi h·ª£p qua network + storage ph√¢n t√°n.
    - L·ª£i: scale t·ªët, fault isolation; H·∫°i: c·∫ßn gi·∫£i quy·∫øt consistency/partitioning.

### Redundancy & Failover Patterns

- [ ] ƒê·ªçc v·ªÅ "Active-Active" redundancy pattern (deep dive)
    - **Active-Active**: nhi·ªÅu instance c√πng nh·∫≠n traffic; LB ph√¢n ph·ªëi; c·∫ßn stateless ho·∫∑c shared state.
    - ∆Øu: t·∫≠n d·ª•ng t√†i nguy√™n, failover nhanh; Nh∆∞·ª£c: ph·ª©c t·∫°p (consistency, split-brain ·ªü data layer).
- [ ] ƒê·ªçc v·ªÅ "Active-Passive" redundancy pattern (deep dive)
    - **Active-Passive**: 1 active ph·ª•c v·ª•, 1+ standby kh√¥ng nh·∫≠n traffic (ho·∫∑c minimal).
    - ∆Øu: ƒë∆°n gi·∫£n h∆°n; Nh∆∞·ª£c: l√£ng ph√≠, failover c√≥ RTO, c·∫ßn c∆° ch·∫ø promote.
- [ ] ƒê·ªçc v·ªÅ "Active-Passive Hot Standby" vs "Active-Passive Cold Standby"
    - **Hot standby**: standby ch·∫°y s·∫µn, sync state/replication, failover nhanh (RTO th·∫•p).
    - **Cold standby**: standby t·∫Øt/√≠t t√†i nguy√™n, kh·ªüi ƒë·ªông khi s·ª± c·ªë (RTO cao, r·∫ª h∆°n).
- [ ] Compare: Active-Active vs Active-Passive (cost, complexity, failover time, use cases)
    - **Cost**: Active-Active cao h∆°n (run full capacity nhi·ªÅu n∆°i) vs Active-Passive th·∫•p h∆°n n·∫øu passive nh·ªè.
    - **Complexity**: Active-Active cao (data consistency, routing) vs Active-Passive th·∫•p h∆°n.
    - **Failover time**: Active-Active th∆∞·ªùng g·∫ßn nh∆∞ t·ª©c th√¨; Active-Passive ph·ª• thu·ªôc detection + promote.
    - **Use cases**:
        - Active-Active: stateless app tier, CDN, multi-AZ web tier.
        - Active-Passive: primary-replica DB, m·ªôt s·ªë h·ªá th·ªëng legacy.
- [ ] ƒê·ªçc v·ªÅ "failover mechanisms": Automatic vs Manual
    - Automatic: nhanh, gi·∫£m downtime; c·∫ßn test k·ªπ ƒë·ªÉ tr√°nh failover ‚Äúfalse positive‚Äù.
    - Manual: an to√†n h∆°n trong v√†i h·ªá, nh∆∞ng downtime l·ªõn v√† ph·ª• thu·ªôc con ng∆∞·ªùi.
- [ ] ƒê·ªçc v·ªÅ "failover time" - RTO (Recovery Time Objective)
    - **RTO**: th·ªùi gian t·ªëi ƒëa ch·∫•p nh·∫≠n ƒë·ªÉ kh√¥i ph·ª•c d·ªãch v·ª• sau s·ª± c·ªë (downtime budget).
- [ ] ƒê·ªçc v·ªÅ "data loss" - RPO (Recovery Point Objective)
    - **RPO**: m·ª©c d·ªØ li·ªáu t·ªëi ƒëa c√≥ th·ªÉ m·∫•t (ƒë·ªô tr·ªÖ replication/backup), v√≠ d·ª• RPO=1 ph√∫t nghƒ©a l√† c√≥ th·ªÉ m·∫•t t·ªëi ƒëa 1 ph√∫t data.
- [ ] Calculate: N·∫øu RTO = 5 minutes, c√≥ nghƒ©a l√† g√¨?
    - Nghƒ©a l√† khi s·ª± c·ªë x·∫£y ra, h·ªá th·ªëng ph·∫£i **kh√¥i ph·ª•c ph·ª•c v·ª•** trong ‚â§ 5 ph√∫t (t·ª´ g√≥c nh√¨n ng∆∞·ªùi d√πng/SLA).
- [ ] Calculate: N·∫øu RPO = 1 minute, c√≥ nghƒ©a l√† g√¨?
    - Nghƒ©a l√† c√≥ th·ªÉ ch·∫•p nh·∫≠n m·∫•t t·ªëi ƒëa 1 ph√∫t d·ªØ li·ªáu g·∫ßn nh·∫•t (do replication/backup lag).
- [ ] ƒê·ªçc v·ªÅ "split-brain problem" trong Active-Active
    - Split-brain: network partition khi·∫øn 2 ph√≠a ƒë·ªÅu nghƒ© m√¨nh ‚Äúprimary‚Äù, d·∫´n t·ªõi writes conflict/corruption n·∫øu kh√¥ng c√≥ consensus/quorum.
- [ ] ƒê·ªçc v·ªÅ "quorum" v√† "consensus" trong distributed systems
    - Quorum: quy·∫øt ƒë·ªãnh d·ª±a tr√™n ƒëa s·ªë (N/2+1) ƒë·ªÉ tr√°nh split-brain.
    - Consensus (Raft/Paxos): ƒë·∫£m b·∫£o agreement v·ªÅ leader/log gi·ªØa c√°c node.
- [ ] Research: How does database replication handle failover? (Master-Slave)
    - M√¥ h√¨nh ph·ªï bi·∫øn: primary-replica; failover c·∫ßn:
        - Detect primary down (health checks/timeout).
        - Promote replica l√™n primary (tooling).
        - Re-point clients/LB/DNS t·ªõi primary m·ªõi.
        - Handle replication lag ‚Üí li√™n quan RPO.
- [ ] Research: How does application-level failover work?
    - App tier: nhi·ªÅu instances + LB health check.
    - Circuit breaker/retry + timeout ƒë·ªÉ tr√°nh ‚Äúcascading failure‚Äù.
    - Readiness/liveness ƒë·ªÉ LB/k8s lo·∫°i instance x·∫•u.

### Zero-Downtime Deployment

- [ ] ƒê·ªçc v·ªÅ "blue-green deployment" strategy
    - Blue (old) & Green (new) ch·∫°y song song; switch traffic m·ªôt l·∫ßn (DNS/LB).
- [ ] ƒê·ªçc v·ªÅ "canary deployment" strategy
    - ƒê·∫©y m·ªôt ph·∫ßn nh·ªè traffic v√†o version m·ªõi, tƒÉng d·∫ßn n·∫øu ·ªïn; rollback nhanh.
- [ ] ƒê·ªçc v·ªÅ "rolling deployment" strategy
    - Thay d·∫ßn t·ª´ng batch instance; c·∫ßn backward-compatible.
- [ ] Compare: Blue-Green vs Canary vs Rolling (cost, risk, complexity, downtime)
    - **Blue-Green**: downtime ~0; cost cao (2 m√¥i tr∆∞·ªùng), rollback nhanh; risk ‚Äúbig bang‚Äù khi switch.
    - **Canary**: cost v·ª´a; risk th·∫•p v√¨ th·ª≠ √≠t traffic; c·∫ßn metrics/observability t·ªët.
    - **Rolling**: cost th·∫•p; risk trung b√¨nh; rollback c√≥ th·ªÉ ch·∫≠m n·∫øu ƒë√£ rollout nhi·ªÅu.
- [ ] ƒê·ªçc v·ªÅ "feature flags" trong zero-downtime deployments
    - Deploy code t√°ch kh·ªèi release feature; b·∫≠t/t·∫Øt theo cohort; rollback feature nhanh kh√¥ng c·∫ßn deploy l·∫°i.
- [ ] ƒê·ªçc v·ªÅ "database migrations" trong zero-downtime deployments
    - Nguy√™n t·∫Øc: **expand-and-contract**:
        - Expand: add columns/tables m·ªõi, code h·ªó tr·ª£ c·∫£ c≈© & m·ªõi.
        - Migrate: backfill.
        - Contract: remove c·ªôt c≈© sau khi ch·∫Øc ch·∫Øn.
- [ ] Analyze: Backward compatibility requirements
    - API: versioning/optional fields.
    - DB: schema changes kh√¥ng breaking khi c√≤n instance old/new c√πng ch·∫°y.
- [ ] ƒê·ªçc v·ªÅ "traffic shifting" strategies
    - Weighted target groups, header-based routing, region-based, time-based.
- [ ] ƒê·ªçc v·ªÅ "rollback strategies" - how to rollback quickly?
    - Fast rollback = switch traffic v·ªÅ version c≈© (blue-green) ho·∫∑c set weight=0 (canary).
    - Data rollback kh√≥ h∆°n code rollback ‚Üí c·∫ßn migration strategy.
- [ ] Research: Kubernetes deployment strategies
    - RollingUpdate, blue-green/canary qua Argo Rollouts/Flagger, service mesh (Istio/Linkerd).
- [ ] Research: Spring Boot zero-downtime deployment patterns
    - Graceful shutdown + readiness probes.
    - Connection draining ·ªü LB.
    - Backward-compatible API/DB changes.

### Capacity Planning Basics

- [ ] ƒê·ªçc v·ªÅ "capacity planning" process (5 steps)
    1. ƒêo baseline (QPS/latency/CPU/memory/IO).
    2. X√°c ƒë·ªãnh SLO/SLA + headroom.
    3. D·ª± b√°o growth + seasonality.
    4. T√≠nh capacity theo bottleneck (CPU/DB/IO/network).
    5. L·∫≠p k·∫ø ho·∫°ch scale + monitoring/alerts + load test ƒë·ªãnh k·ª≥.
- [ ] ƒê·ªçc v·ªÅ "baseline measurement" - current capacity
    - ƒêo ·ªü peak/average: QPS, P95/P99, saturation (CPU steal, GC), DB connections, cache hit rate.
- [ ] ƒê·ªçc v·ªÅ "growth projection" - future capacity needs
    - D·ª±a tr√™n MAU/DAU, conversion, feature changes; d√πng m√¥ h√¨nh worst-case + seasonal peaks.
- [ ] ƒê·ªçc v·ªÅ "headroom" - buffer capacity
    - Headroom 20‚Äì50% tu·ª≥ h·ªá; payment/realtime th∆∞·ªùng c·∫ßn headroom cao h∆°n.
- [ ] Calculate: N·∫øu current load = 1K QPS, growth = 20%/month, c·∫ßn capacity sau 6 th√°ng?
    - C√¥ng th·ª©c: \(QPS_{6} = 1000 \times 1.2^6 \approx 1000 \times 2.986 = 2986\) QPS.
    - N·∫øu headroom 20%: c·∫ßn \(\approx 2986 \times 1.2 \approx 3583\) QPS capacity.
- [ ] ƒê·ªçc v·ªÅ "capacity units" - how to measure?
    - QPS/RPS, concurrent users/connections, bytes/s, DB TPS, cache ops/s, queue lag, compute units.
- [ ] ƒê·ªçc v·ªÅ "scaling triggers" - when to scale?
    - CPU > 60‚Äì70% sustained, P95 latency v∆∞·ª£t SLO, queue lag tƒÉng, DB connections/IO saturation.
- [ ] ƒê·ªçc v·ªÅ "auto-scaling" vs "manual scaling"
    - Auto: ph·∫£n ·ª©ng nhanh, c·∫ßn guardrails; Manual: ki·ªÉm so√°t t·ªët nh∆∞ng ch·∫≠m.
- [ ] Research: AWS Auto Scaling groups
    - ASG + scaling policies (target tracking, step scaling) + health checks + warm-up.
- [ ] Research: Kubernetes Horizontal Pod Autoscaler (HPA)
    - HPA theo CPU/memory/custom metrics; c·∫ßn request/limit ƒë√∫ng; k·∫øt h·ª£p VPA/Cluster Autoscaler khi c·∫ßn.

---

## Design TODOs

### Design Exercise 1: Payment Gateway - Horizontal Scaling

- [ ] Thi·∫øt k·∫ø Payment Gateway v·ªõi horizontal scaling requirement
- [ ] Requirement: Scale from 1K QPS to 50K QPS
- [ ] Design: Load balancer architecture (type, algorithm, placement)
- [ ] Design: Application server scaling strategy
- [ ] Design: Database scaling strategy (read replicas, sharding consideration)
- [ ] Design: Stateless service architecture (no session state)
- [ ] Identify: All stateful components (n·∫øu c√≥)
- [ ] Design: Solution ƒë·ªÉ make stateful components stateless
- [ ] Calculate: Number of servers needed (assume 5K QPS per server)
- [ ] Calculate: Cost comparison - vertical vs horizontal scaling
- [ ] Design: Auto-scaling rules (when to scale up/down)
- [ ] Document: Complete architecture diagram v·ªõi scaling paths
- [ ] Document: Design decisions v√† trade-offs (500 words)

### Design Exercise 2: Betting Platform - High Availability

- [ ] Thi·∫øt k·∫ø Betting Platform v·ªõi 99.99% availability requirement
- [ ] Design: Multi-layer redundancy (application, database, load balancer)
- [ ] Design: Active-Active setup cho application servers
- [ ] Design: Active-Passive setup cho database (Master-Slave)
- [ ] Design: Failover mechanism (automatic, < 30 seconds)
- [ ] Design: Health check strategy (what to check, how often)
- [ ] Design: Monitoring v√† alerting (what to monitor, alert thresholds)
- [ ] Identify: All SPOFs trong design
- [ ] Eliminate: All SPOFs (redesign n·∫øu c·∫ßn)
- [ ] Design: Disaster recovery plan (RTO = 5 minutes, RPO = 1 minute)
- [ ] Design: Multi-region deployment (optional, nh∆∞ng n√™n consider)
- [ ] Calculate: Availability c·ªßa to√†n b·ªô system (series v√† parallel components)
- [ ] Document: Failure scenarios (√≠t nh·∫•t 5 scenarios) v√† mitigation
- [ ] Document: Complete HA architecture (500 words)

### Design Exercise 3: Wallet System - Stateless Architecture

- [ ] Thi·∫øt k·∫ø Wallet System v·ªõi stateless requirement
- [ ] Requirement: Must be horizontally scalable, no session state
- [ ] Design: Authentication strategy (stateless - JWT)
- [ ] Design: Session management (n·∫øu c·∫ßn, d√πng external store)
- [ ] Design: State storage (Redis cho shared state)
- [ ] Identify: All stateful operations
- [ ] Design: Solution ƒë·ªÉ make operations stateless
- [ ] Design: Load balancer configuration (algorithm, sticky sessions?)
- [ ] Design: Cache strategy (shared cache, not in-memory)
- [ ] Design: Database connection strategy (connection pooling, no local state)
- [ ] Verify: System c√≥ th·ªÉ scale horizontally kh√¥ng? (checklist)
- [ ] Document: Stateless architecture design (500 words)

### Design Exercise 4: Zero-Downtime Deployment Strategy

- [ ] Design: Deployment strategy cho Payment Gateway
- [ ] Requirement: Zero downtime, support rollback
- [ ] Choose: Blue-Green, Canary, or Rolling deployment
- [ ] Justify: T·∫°i sao ch·ªçn strategy ƒë√≥?
- [ ] Design: Traffic routing during deployment
- [ ] Design: Database migration strategy (backward compatible)
- [ ] Design: Feature flag strategy (n·∫øu c·∫ßn)
- [ ] Design: Rollback procedure (step-by-step)
- [ ] Design: Health check during deployment
- [ ] Design: Monitoring during deployment (what to watch)
- [ ] Estimate: Deployment time v√† risk
- [ ] Document: Complete deployment strategy (500 words)

### Design Exercise 5: Capacity Planning

- [ ] Scenario: E-commerce platform, current 10K users, expected 1M users in 1 year
- [ ] Measure: Current capacity (QPS, storage, bandwidth) - estimate n·∫øu kh√¥ng c√≥ data
- [ ] Project: Future capacity needs (1M users)
- [ ] Calculate: Growth rate (monthly)
- [ ] Calculate: Peak capacity needed (assume 10x average)
- [ ] Design: Scaling plan (when to add servers)
- [ ] Calculate: Server count needed (assume 5K QPS per server)
- [ ] Calculate: Storage needed (assume 1GB per 1K users)
- [ ] Calculate: Bandwidth needed (assume 5KB per request)
- [ ] Design: Auto-scaling rules (triggers, min/max instances)
- [ ] Estimate: Cost projection (monthly, yearly)
- [ ] Design: Monitoring ƒë·ªÉ track capacity usage
- [ ] Document: Complete capacity planning document

---

## Coding TODOs

### Task 1: Stateless Spring Boot Application

- [ ] T·∫°o Spring Boot project: Stateless Payment API
- [ ] Implement: REST API endpoints (no session state)
- [ ] Implement: JWT-based authentication (stateless)
- [ ] Verify: No in-memory session storage
- [ ] Verify: No server-side session state
- [ ] Test: Deploy 2 instances, verify both can handle same user
- [ ] Test: Kill one instance, verify other instance continues working
- [ ] Document: Stateless design decisions

### Task 2: Load Balancer Implementation

- [ ] Implement: Simple load balancer trong Java
- [ ] Algorithm: Round-robin
- [ ] Algorithm: Weighted round-robin (add weights)
- [ ] Algorithm: Least connections (track connection count)
- [ ] Add: Health check mechanism (ping backend)
- [ ] Add: Remove unhealthy backends automatically
- [ ] Add: Add back healthy backends automatically
- [ ] Test: With 3 backend servers
- [ ] Test: Mark one server unhealthy, verify traffic stops
- [ ] Test: Mark server healthy again, verify traffic resumes
- [ ] Measure: Load balancer overhead (latency added)
- [ ] Document: Code v√† test results

### Task 3: Health Check Implementation

- [ ] Implement: `/health` endpoint (basic)
- [ ] Implement: `/health/readiness` endpoint
- [ ] Implement: `/health/liveness` endpoint
- [ ] Add: Database connection check trong readiness
- [ ] Add: External service check trong readiness
- [ ] Add: Memory check trong liveness (fail if > 90%)
- [ ] Add: Disk space check trong liveness (fail if > 90%)
- [ ] Add: Thread pool check (fail if all threads busy)
- [ ] Test: All health endpoints
- [ ] Test: Simulate DB down, verify readiness fails
- [ ] Test: Simulate high memory, verify liveness fails
- [ ] Integrate: Health checks v·ªõi load balancer
- [ ] Document: Health check strategy

### Task 4: Circuit Breaker v·ªõi Resilience4j

- [ ] Add: Resilience4j dependency
- [ ] Implement: Circuit breaker cho external API call
- [ ] Configure: Failure threshold (50% failures)
- [ ] Configure: Wait duration (60 seconds)
- [ ] Configure: Half-open state (allow 3 requests)
- [ ] Add: Fallback mechanism
- [ ] Test: Normal operation (circuit closed)
- [ ] Test: Simulate failures, verify circuit opens
- [ ] Test: Verify fallback ƒë∆∞·ª£c called khi circuit open
- [ ] Test: Wait duration, verify circuit goes half-open
- [ ] Test: Successful requests in half-open, verify circuit closes
- [ ] Monitor: Circuit breaker metrics
- [ ] Document: Circuit breaker configuration v√† behavior

### Task 5: Retry v·ªõi Exponential Backoff

- [ ] Implement: Retry mechanism cho failed requests
- [ ] Algorithm: Exponential backoff (1s, 2s, 4s, 8s)
- [ ] Add: Max retry attempts (3)
- [ ] Add: Jitter (random delay to prevent thundering herd)
- [ ] Test: Transient failure, verify retry works
- [ ] Test: Permanent failure, verify stops after max retries
- [ ] Test: Success after retry, verify no more retries
- [ ] Measure: Total time v·ªõi retries
- [ ] Integrate: Retry v·ªõi circuit breaker
- [ ] Document: Retry strategy

### Task 6: Multi-Instance Deployment

- [ ] Build: Spring Boot application JAR
- [ ] Deploy: 2 instances tr√™n local (different ports)
- [ ] Setup: Nginx nh∆∞ load balancer (ho·∫∑c use Spring Cloud Gateway)
- [ ] Configure: Round-robin load balancing
- [ ] Configure: Health checks
- [ ] Test: Send 100 requests, verify distributed across instances
- [ ] Test: Kill one instance, verify traffic goes to other
- [ ] Test: Restart killed instance, verify traffic resumes
- [ ] Monitor: Request distribution (logs)
- [ ] Document: Deployment setup

### Task 7: State Externalization (Redis)

- [ ] Setup: Redis server
- [ ] Implement: Session storage trong Redis (thay v√¨ in-memory)
- [ ] Implement: Cache trong Redis (thay v√¨ local cache)
- [ ] Test: Deploy 2 instances, verify shared state (Redis)
- [ ] Test: User login on instance 1, verify session available on instance 2
- [ ] Test: Kill instance 1, verify user still logged in on instance 2
- [ ] Measure: Redis latency impact
- [ ] Document: State externalization strategy

### Task 8: Graceful Shutdown

- [ ] Implement: Graceful shutdown trong Spring Boot
- [ ] Add: Shutdown hook ƒë·ªÉ stop accepting new requests
- [ ] Add: Wait for in-flight requests to complete
- [ ] Add: Timeout (30 seconds max wait)
- [ ] Test: Send requests, then shutdown app
- [ ] Verify: In-flight requests complete before shutdown
- [ ] Verify: New requests rejected during shutdown
- [ ] Test: With load balancer (remove from pool during shutdown)
- [ ] Document: Graceful shutdown implementation

---

## Failure & Resilience TODOs

### Failure Simulation 1: Server Failure

- [ ] Setup: 3 application instances behind load balancer
- G·ª£i √Ω setup: 3 instance (A/B/C) + 1 LB (Nginx/HAProxy/ALB). M·ªói instance expose `/health` + log `instance_id`.
- [ ] Test: Normal operation (all healthy)
- K·ª≥ v·ªçng: ph√¢n ph·ªëi traffic g·∫ßn ƒë·ªÅu (RR) ho·∫∑c theo thu·∫≠t to√°n ƒë√£ ch·ªçn; error rate ~0; P95 ·ªïn ƒë·ªãnh.
- [ ] Simulate: Kill one instance abruptly
- C√°ch l√†m: kill process/container c·ªßa instance B ngay l·∫≠p t·ª©c (kh√¥ng graceful).
- [ ] Measure: Time for load balancer to detect failure
- ƒêo: t·ª´ l√∫c kill ƒë·∫øn l√∫c health-check chuy·ªÉn `fail`. Ph·ª• thu·ªôc interval/timeout/rise/fall (v√≠ d·ª• 5s interval, 2 fails ‚áí ~10‚Äì15s).
- [ ] Measure: Time for traffic to stop going to dead instance
- ƒêo: t·ª´ l√∫c kill ƒë·∫øn l√∫c LB kh√¥ng c√≤n route v√†o B (quan s√°t logs/LB stats).
- [ ] Measure: Impact on users (errors, latency)
- K·ª≥ v·ªçng: c√≥ th·ªÉ c√≥ m·ªôt spike l·ªói ng·∫Øn (trong c·ª≠a s·ªï detect), sau ƒë√≥ v·ªÅ b√¨nh th∆∞·ªùng; latency c√≥ th·ªÉ tƒÉng nh·∫π do retry.
- [ ] Verify: Other instances handle increased load
- K·ª≥ v·ªçng: A/C CPU tƒÉng, nh∆∞ng v·∫´n n·∫±m d∆∞·ªõi threshold SLO.
- [ ] Restore: Dead instance
- Kh·ªüi ƒë·ªông l·∫°i B; ƒë·∫£m b·∫£o warm-up/readiness pass tr∆∞·ªõc khi nh·∫≠n traffic.
- [ ] Measure: Time for instance to rejoin pool
- ƒêo: t·ª´ l√∫c start ƒë·∫øn khi LB ƒë∆∞a B v√†o l·∫°i (rise count + readiness).
- [ ] Document: Failure scenario v√† recovery
- Ghi: c·∫•u h√¨nh health-check, timeline, s·ªë request l·ªói, lesson learned (tuning interval, retries, graceful shutdown).

### Failure Simulation 2: Database Failure

- [ ] Setup: Application v·ªõi database
- Setup t·ªëi thi·ªÉu: app c√≥ endpoint truy DB th·∫≠t (SELECT 1 / query ƒë∆°n gi·∫£n) + readiness check ki·ªÉm DB connection.
- [ ] Test: Normal operation
- K·ª≥ v·ªçng: readiness OK; error rate th·∫•p; DB connections ·ªïn ƒë·ªãnh.
- [ ] Simulate: Database connection failure
- C√°ch l√†m: t·∫Øt DB, drop network (iptables/security group), ho·∫∑c ƒë·ªïi credential ƒë·ªÉ fail fast.
- [ ] Verify: Readiness check fails
- K·ª≥ v·ªçng: `/health/readiness` fail nhanh (timeout ng·∫Øn 1‚Äì2s).
- [ ] Verify: Load balancer removes instance from pool
- K·ª≥ v·ªçng: instance b·ªã lo·∫°i kh·ªèi upstream/target group; traffic kh√¥ng v√†o instance ‚Äúƒë√£ m·∫•t DB‚Äù.
- [ ] Verify: Circuit breaker opens (n·∫øu c√≥)
- K·ª≥ v·ªçng: sau m·ªôt s·ªë l·ªói, CB open ƒë·ªÉ b·∫£o v·ªá DB v√† gi·∫£m latency cascade.
- [ ] Restore: Database connection
- B·∫≠t DB l·∫°i/kh√¥i ph·ª•c network.
- [ ] Verify: Readiness check passes
- K·ª≥ v·ªçng: readiness OK sau khi DB ·ªïn ƒë·ªãnh.
- [ ] Verify: Instance rejoins pool
- K·ª≥ v·ªçng: LB add l·∫°i instance theo rise count.
- [ ] Measure: Total downtime
- ƒêo: th·ªùi gian user g·∫∑p l·ªói (n·∫øu c√≥) v√† th·ªùi gian h·ªá t·ª± h·ªìi ph·ª•c.
- [ ] Document: Database failure handling
- Ghi: timeout, retry policy, CB config, c√°ch tr√°nh ‚Äúthundering herd‚Äù khi DB v·ª´a l√™n.

### Failure Simulation 3: Partial Failure

- [ ] Setup: Multi-instance application
- √çt nh·∫•t 2‚Äì3 instances + LB + metrics.
- [ ] Simulate: One instance slow (add artificial delay)
- C√°ch l√†m: th√™m delay 500ms‚Äì2s v√†o 1 instance (B).
- [ ] Verify: Load balancer behavior (least connections?)
- Quan s√°t: RR v·∫´n g·ª≠i ƒë·ªÅu; LeastConn/LeastRT (n·∫øu c√≥) s·∫Ω tr√°nh B theo th·ªùi gian.
- [ ] Measure: Impact on overall latency
- K·ª≥ v·ªçng: n·∫øu RR, P95/P99 tƒÉng; n·∫øu least-response-time, impact th·∫•p h∆°n.
- [ ] Simulate: One instance high error rate (50%)
- C√°ch l√†m: inject l·ªói 5xx ng·∫´u nhi√™n tr√™n B.
- [ ] Verify: Circuit breaker behavior
- K·ª≥ v·ªçng: CB ·ªü client/app tier gi·∫£m s·ªë call v√†o B (n·∫øu B l√† downstream) ho·∫∑c gi·∫£m call downstream n·∫øu downstream l·ªói.
- [ ] Verify: Load balancer removes unhealthy instance
- N·∫øu health-check d·ª±a tr√™n error rate/active check: B b·ªã lo·∫°i; n·∫øu ch·ªâ TCP health-check th√¨ c√≥ th·ªÉ v·∫´n gi·ªØ ‚áí c·∫ßn c·∫•u h√¨nh health-check ƒë√∫ng.
- [ ] Document: Partial failure handling
- Ghi: ch·ªçn thu·∫≠t to√°n LB, health-check ki·ªÉu g√¨ (active HTTP check vs passive), ng∆∞·ª°ng lo·∫°i instance.

### Failure Simulation 4: Network Partition

- [ ] Setup: 2 instances, shared Redis
- Redis d√πng cho session/cache/presence; c·∫£ 2 instance ƒë·ªçc/ghi Redis.
- [ ] Simulate: Network partition (instance 1 can't reach Redis)
- Ch·∫∑n network t·ª´ instance1 ‚Üí Redis.
- [ ] Verify: Instance 1 behavior (fail fast? retry?)
- Best practice: fail fast + limited retry + degrade (n·∫øu c√≥) ƒë·ªÉ tr√°nh treo thread.
- [ ] Verify: Instance 2 continues working
- K·ª≥ v·ªçng: instance2 v·∫´n OK v√¨ c√≤n access Redis.
- [ ] Verify: Load balancer removes instance 1
- N·∫øu readiness check include Redis dependency: instance1 readiness fail ‚áí LB lo·∫°i.
- [ ] Restore: Network connectivity
- M·ªü l·∫°i network.
- [ ] Verify: Instance 1 recovers
- K·ª≥ v·ªçng: readiness pass; LB add l·∫°i sau rise.
- [ ] Document: Network partition handling
- Ghi: dependency checks n√†o ƒë∆∞a v√†o readiness, timeout/retry/jitter, b√†i h·ªçc v·ªÅ cascading failure.

### Resilience Testing

- [ ] Implement: Chaos testing scenario
- K·ªãch b·∫£n ƒë∆°n gi·∫£n: m·ªói 30s kill ng·∫´u nhi√™n 1 instance; ho·∫∑c inject latency/error; gi·ªØ load ·ªïn ƒë·ªãnh.
- [ ] Test: Random instance failures (kill random instance every 30s)
- Ch·∫°y 10‚Äì30 ph√∫t ƒë·ªÉ th·∫•y xu h∆∞·ªõng, kh√¥ng ch·ªâ 1‚Äì2 l·∫ßn.
- [ ] Measure: System behavior under chaos
- Track: error rate, P95/P99 latency, recovery time, autoscaling events.
- [ ] Verify: System continues serving requests
- Target: kh√¥ng v∆∞·ª£t SLO/SLA qu√° nhi·ªÅu; kh√¥ng ‚Äúglobal outage‚Äù.
- [ ] Measure: Error rate during chaos
- Ghi theo th·ªùi gian; correlate v·ªõi failover detection windows.
- [ ] Measure: Latency during chaos
- Quan s√°t tail latency; latency spike th∆∞·ªùng do retry/connection churn.
- [ ] Document: Resilience test results
- T√≥m t·∫Øt: timeline, charts, bottlenecks, config c·∫ßn tune.
- [ ] Identify: Weak points trong system
- Th∆∞·ªùng g·∫∑p: DB/Redis SPOF, timeout qu√° d√†i, retry kh√¥ng jitter, health-check sai.
- [ ] Propose: Improvements ƒë·ªÉ handle chaos better
- Timeout budgets, retry w/ jitter, CB, bulkheads, better readiness, autoscaling thresholds, multi-AZ.

---

## Analysis TODOs

### Analysis Task 1: Scaling Strategy Decision

- [ ] Scenario: Current system handles 1K QPS, need to handle 10K QPS
- Goal: tƒÉng 10x; ∆∞u ti√™n horizontal + stateless.
- [ ] Option 1: Vertical scaling (bigger server)
- V√≠ d·ª•: tƒÉng CPU/RAM 8‚Äì16x; c√≥ l·ª£i n·∫øu bottleneck compute ƒë∆°n gi·∫£n.
- [ ] Calculate: Cost c·ªßa vertical scaling
- Quy t·∫Øc: m√°y c√†ng l·ªõn chi ph√≠ tƒÉng kh√¥ng tuy·∫øn t√≠nh; th√™m risk downtime khi resize (tu·ª≥ platform).
- [ ] Identify: Limitations (max server size)
- Gi·ªõi h·∫°n: max instance type, NUMA/GC, single disk IO ceiling, single network interface.
- [ ] Identify: Single point of failure risk
- 1 m√°y ch·∫øt = outage (tr·ª´ khi c√≥ HA/failover) ‚áí SPOF.
- [ ] Option 2: Horizontal scaling (more servers)
- V√≠ d·ª•: t·ª´ 2 instances l√™n 20 instances (tu·ª≥ QPS/instance).
- [ ] Calculate: Cost c·ªßa horizontal scaling
- Cost = N * instance + LB + ops; th∆∞·ªùng tuy·∫øn t√≠nh h∆°n; c√≥ th·ªÉ t·ªëi ∆∞u spot/reserved.
- [ ] Identify: Complexity added
- LB, statelessness, shared state, distributed tracing, deploy orchestration.
- [ ] Identify: Load balancer requirement
- C·∫ßn L4/L7 LB + health checks + connection draining.
- [ ] Compare: Vertical vs Horizontal (cost, complexity, risk, scalability)
- K·∫øt lu·∫≠n chung: 10x th∆∞·ªùng n√™n **horizontal**; vertical ch·ªâ l√† b∆∞·ªõc ƒë·∫ßu/ƒë·ªám.
- [ ] Recommend: Which strategy? Justify v·ªõi numbers
- M·∫´u: n·∫øu 1 instance x·ª≠ l√Ω 500 QPS an to√†n, 10K QPS c·∫ßn 20 instances + 20% headroom ‚áí 24 instances.
- [ ] Document: Decision analysis (500 words)
- N√™u bottleneck, gi·∫£ ƒë·ªãnh, c√¥ng th·ª©c, trade-offs.

### Analysis Task 2: Load Balancer Algorithm Selection

- [ ] Scenario: Payment API, different servers have different capacities
- Server c√≥ CPU kh√°c nhau ho·∫∑c autoscaling mixed types.
- [ ] Analyze: Round-robin suitability
- RR t·ªët khi backend ƒë·ªìng nh·∫•t + request cost g·∫ßn gi·ªëng nhau; kh√¥ng t·ªët khi heterogenous.
- [ ] Analyze: Weighted round-robin suitability
- WRR ph√π h·ª£p heterogenous: weight theo capacity (CPU, QPS benchmark).
- [ ] Analyze: Least connections suitability
- H·ª£p v·ªõi request th·ªùi gian x·ª≠ l√Ω kh√°c nhau/connection d√†i; l∆∞u √Ω keep-alive c√≥ th·ªÉ l√†m l·ªách.
- [ ] Analyze: IP hash suitability (sticky sessions needed?)
- IP hash d√πng cho session affinity; payment API n√™n stateless ‚áí th∆∞·ªùng **kh√¥ng c·∫ßn** sticky.
- [ ] Test: Each algorithm v·ªõi realistic load
- D√πng k6/JMeter; mix endpoint n·∫∑ng/nh·∫π.
- [ ] Measure: Load distribution (even? uneven?)
- So s√°nh RPS/instance, CPU, queue time.
- [ ] Measure: Latency impact
- Track P95/P99 theo thu·∫≠t to√°n.
- [ ] Recommend: Best algorithm cho scenario
- G·ª£i √Ω: **Weighted RR** (capacity-aware) + health checks; n·∫øu workload bi·∫øn thi√™n m·∫°nh, c√¢n nh·∫Øc least-time/latency-aware (tu·ª≥ LB).
- [ ] Justify: Recommendation v·ªõi data
- Ch·ªët b·∫±ng s·ªë li·ªáu t·ª´ test.
- [ ] Document: Algorithm selection analysis
- Ghi assumptions, graphs, config.

### Analysis Task 3: Stateless vs Stateful Trade-off

- [ ] Scenario: E-commerce cart system
- Cart l√† state theo user, c·∫ßn t·ªìn t·∫°i qua nhi·ªÅu request.
- [ ] Option 1: Stateless (cart in Redis)
- Pattern: app stateless; cart stored Redis (key=userId), TTL + persistence (AOF/RDB) tu·ª≥ y√™u c·∫ßu.
- [ ] Analyze: Pros (3 points)
    1. Scale app d·ªÖ, kh√¥ng c·∫ßn sticky session.
    2. HA t·ªët h∆°n (instance ch·∫øt kh√¥ng m·∫•t cart).
    3. Deploy/failover ƒë∆°n gi·∫£n h∆°n.
- [ ] Analyze: Cons (3 points)
    1. Redis tr·ªü th√†nh dependency quan tr·ªçng (c·∫ßn HA).
    2. Th√™m latency m·∫°ng + chi ph√≠.
    3. Consistency/locking c·∫ßn thi·∫øt n·∫øu multi-write.
- [ ] Analyze: Complexity
- C·∫ßn schema key, TTL, idempotency, cache stampede control.
- [ ] Option 2: Stateful (cart in server memory)
- Pattern: sticky sessions ho·∫∑c consistent routing theo user.
- [ ] Analyze: Pros (3 points)
    1. Latency c·ª±c th·∫•p (local memory).
    2. ƒê∆°n gi·∫£n khi ch·ªâ 1 instance.
    3. Gi·∫£m ph·ª• thu·ªôc external store.
- [ ] Analyze: Cons (3 points)
    1. M·∫•t cart khi instance ch·∫øt/restart.
    2. Scale kh√≥, c·∫ßn sticky, m·∫•t c√¢n b·∫±ng t·∫£i.
    3. Rolling deploy/auto-scale ph·ª©c t·∫°p.
- [ ] Analyze: Scalability limitations
- Tr·∫ßn scale theo memory/instance; kh√≥ redistribute cart state khi scale.
- [ ] Compare: Stateless vs Stateful
- Production ph·ªï bi·∫øn: **stateless app + Redis/DB** (state external).
- [ ] Recommend: Which approach? Justify
- Khuy·∫øn ngh·ªã: stateless + Redis; n·∫øu c·∫ßn durability cao, ghi DB (event sourcing ho·∫∑c periodic snapshot).
- [ ] Document: Trade-off analysis (500 words)
- N√™u RPO/RTO, cost, operability.

### Analysis Task 4: Availability Calculation

- [ ] System c√≥ 5 components: Load Balancer (99.9%), App Server (99.9%), Database (99.95%), Cache (99.9%), External
  API (99%)
- [ ] Calculate: Overall availability (all in series)
    - Series availability: nh√¢n c√°c availability:
      \[
      A = 0.999 \times 0.999 \times 0.9995 \times 0.999 \times 0.99 \approx 0.9865
      \]
      ‚áí **~98.65%** (downtime ~ 4.9 ng√†y/nƒÉm).
- [ ] Scenario: App Servers c√≥ 3 instances (parallel, each 99.9%)
- [ ] Calculate: App Server cluster availability
    - Parallel (√≠t nh·∫•t 1 s·ªëng): \(A_{cluster} = 1-(1-0.999)^3 = 1-10^{-9} = 0.999999999\) (~ ‚Äú9 s·ªë 9‚Äù).
- [ ] Scenario: Database c√≥ Master-Slave (active-passive)
- [ ] Calculate: Database availability
    - N·∫øu gi·∫£ ƒë·ªãnh ƒë·ªôc l·∫≠p v√† failover ‚Äúho√†n h·∫£o‚Äù: \(A \approx 1-(1-0.9995)^2 = 0.99999975\).
    - Th·ª±c t·∫ø th·∫•p h∆°n v√¨ failover kh√¥ng ho√†n h·∫£o + replication lag (RPO) + operator errors.
- [ ] Recalculate: Overall system availability v·ªõi redundancy
    - Thay app cluster v√† DB HA v√†o:
      \(A \approx 0.999(LB)\times 0.999999999(app)\times 0.99999975(DB)\times 0.999(cache)\times 0.99(ext)\)
      \(\approx 0.9880\) ‚áí v·∫´n b·ªã k√©o xu·ªëng b·ªüi **External API 99%**.
- [ ] Analyze: Which component is weakest link?
    - Weakest link: **External API 99%** (v√† c√°c dependency ‚Äúseries‚Äù kh√°c).
- [ ] Propose: How to improve weakest component?
    - Th√™m cache/fallback, degrade mode, async queue, multi-provider, bulkheads + timeouts.
- [ ] Calculate: New overall availability after improvement
    - V√≠ d·ª•: thay External API b·∫±ng 99.9% hi·ªáu d·ª•ng nh·ªù cache/fallback:
      t·ªïng availability tƒÉng x·∫•p x·ªâ theo t·ªâ l·ªá (0.999/0.99) ‚âà 1.009 ‚áí c·∫£i thi·ªán ƒë√°ng k·ªÉ.
- [ ] Document: Availability analysis v√† improvements
    - N√™u r√µ gi·∫£ ƒë·ªãnh (independence, failover time), v√† c√°ch ƒëo th·∫≠t (SLO-based).

### Analysis Task 5: Capacity Planning

- [ ] Current: 1K QPS, 2 servers (500 QPS each)
- [ ] Growth: 50% per month
- [ ] Calculate: QPS after 3 months
    - \(QPS_3 = 1000 \times 1.5^3 = 3375\) QPS.
- [ ] Calculate: QPS after 6 months
    - \(QPS_6 = 1000 \times 1.5^6 \approx 11390.6\) QPS.
- [ ] Calculate: QPS after 12 months
    - \(QPS_{12} = 1000 \times 1.5^{12} \approx 129746\) QPS.
- [ ] Plan: When to add servers? (trigger points)
    - Trigger theo SLO: CPU > 65% 10 ph√∫t, P95 > SLO, queue lag tƒÉng; scale tr∆∞·ªõc peak.
- [ ] Calculate: Server count needed each month
    - N·∫øu 1 server an to√†n 500 QPS: c·∫ßn \(N = \lceil QPS/500 \rceil\). V·ªõi 20% headroom: \(N = \lceil 1.2\times QPS/500 \rceil\).
- [ ] Calculate: Cost projection (monthly)
    - Cost ‚âà N * instance_cost + LB + DB/Redis + logs/metrics; t√°ch fixed vs variable.
- [ ] Design: Auto-scaling rules (min, max, scale-up threshold, scale-down threshold)
    - M·∫´u: min=2, max=40; scale up khi CPU>60% 5 ph√∫t ho·∫∑c P95> SLO; scale down khi CPU<30% 15 ph√∫t.
- [ ] Estimate: Headroom needed (20% buffer)
    - Headroom 20% cho web b√¨nh th∆∞·ªùng; payment peak/flash sale n√™n 30‚Äì50% + pre-scale.
- [ ] Document: Capacity planning v√† scaling plan
    - Include: assumptions, peak factor, bottlenecks, test plan.

### Analysis Task 6: Deployment Strategy Comparison

- [ ] Compare: Blue-Green vs Canary vs Rolling deployment
- [ ] Criteria: Downtime, Risk, Cost, Complexity, Rollback speed
- [ ] Score: Each strategy on each criteria (1-10)
    - Tham kh·∫£o (tu·ª≥ h·ªá):
        - Blue-Green: Downtime 9, Risk 6, Cost 4, Complexity 6, Rollback 9
        - Canary: Downtime 9, Risk 8, Cost 6, Complexity 7, Rollback 8
        - Rolling: Downtime 7, Risk 6, Cost 9, Complexity 5, Rollback 6
- [ ] Analyze: Best use case cho Blue-Green
    - Khi c·∫ßn rollback t·ª©c th√¨, release √≠t nh∆∞ng ‚Äúbig‚Äù, c√≥ th·ªÉ ch·∫°y 2 m√¥i tr∆∞·ªùng.
- [ ] Analyze: Best use case cho Canary
    - Khi h·ªá critical, c·∫ßn gi·∫£m blast radius, c√≥ metrics t·ªët, mu·ªën progressive delivery.
- [ ] Analyze: Best use case cho Rolling
    - Khi cost-sensitive, release th∆∞·ªùng xuy√™n, backward-compatible t·ªët.
- [ ] Scenario: Critical payment system
- [ ] Recommend: Deployment strategy
    - Khuy·∫øn ngh·ªã: **Canary** + feature flags + strict SLO monitoring; m·ªôt s·ªë thay ƒë·ªïi l·ªõn d√πng blue-green.
- [ ] Justify: Recommendation
    - Payment c·∫ßn gi·∫£m r·ªßi ro, rollback nhanh, ki·ªÉm so√°t theo cohort.
- [ ] Document: Deployment strategy analysis
    - N√™u pipeline, gating metrics, rollback procedure.

---

## Review TODOs

### Self-Evaluation

- [ ] Review: T·∫•t c·∫£ Study TODOs ho√†n th√†nh ch∆∞a?
- Checklist: ƒë·ªçc xong + c√≥ ghi ch√©p + c√≥ v√≠ d·ª•/mini-lab cho t·ª´ng ph·∫ßn.
- [ ] Review: T·∫•t c·∫£ Design exercises ho√†n th√†nh ch∆∞a?
- (Kh√¥ng ƒëi·ªÅn design ·ªü ƒë√¢y theo y√™u c·∫ßu c·ªßa b·∫°n) Ch·ªâ nh·∫Øc: ph·∫£i c√≥ diagram + trade-offs + assumptions.
- [ ] Review: T·∫•t c·∫£ Coding tasks ƒë√£ code v√† test ch∆∞a?
- (Kh√¥ng ƒëi·ªÅn coding ·ªü ƒë√¢y theo y√™u c·∫ßu c·ªßa b·∫°n) Nh·∫Øc: c√≥ logs ch·ª©ng minh distribution/failover.
- [ ] Review: T·∫•t c·∫£ Failure simulations ƒë√£ run ch∆∞a?
- ƒê·∫£m b·∫£o c√≥ timeline + s·ªë li·ªáu error/latency + config health-check.
- [ ] Review: T·∫•t c·∫£ Analysis tasks ƒë√£ complete ch∆∞a?
- C√°c ph√©p t√≠nh/c√¥ng th·ª©c ph·∫£i r√µ gi·∫£ ƒë·ªãnh (independence, headroom, per-server capacity).
- [ ] Rate: Understanding c·ªßa horizontal scaling (1-10)
- G·ª£i √Ω t·ª± ch·∫•m: 7+ n·∫øu b·∫°n t·ª± t√≠nh ƒë∆∞·ª£c server count + ch·ªâ ra bottleneck + n√≥i ƒë∆∞·ª£c trade-off.
- [ ] Rate: Understanding c·ªßa load balancing (1-10)
- 7+ n·∫øu b·∫°n gi·∫£i th√≠ch ƒë∆∞·ª£c L4 vs L7, sticky sessions, health-check, thu·∫≠t to√°n.
- [ ] Rate: Understanding c·ªßa stateless design (1-10)
- 7+ n·∫øu b·∫°n bi·∫øt externalize state, thi·∫øt k·∫ø JWT/session ƒë√∫ng, v√† x·ª≠ l√Ω WebSocket state.
- [ ] Rate: Understanding c·ªßa high availability (1-10)
- 7+ n·∫øu b·∫°n t√≠nh availability, bi·∫øt RTO/RPO, v√† lo·∫°i SPOF.
- [ ] Rate: Practical skills (implementation) (1-10)
- 7+ n·∫øu b·∫°n t·ª± d·ª±ng 2‚Äì3 instances + LB + health-check + chaos test c∆° b·∫£n.
- [ ] Identify: 3 concepts b·∫°n master
- M·∫´u: (1) L4 vs L7 LB, (2) stateless & externalize state, (3) health-check/readiness/liveness.
- [ ] Identify: 3 concepts c·∫ßn improve
- M·∫´u: (1) DB failover tooling, (2) SLO/SLI & observability, (3) canary + feature flags + safe migrations.
- [ ] Plan: How to improve 3 weak areas
- M·∫´u plan 1 tu·∫ßn: ƒë·ªçc 2 b√†i + l√†m 1 lab + vi·∫øt 1 trang recap cho m·ªói weak area.

### Architecture Review

- [ ] Review: Payment Gateway design
- Focus: SLA/SLO, idempotency, retries/timeouts, rate limiting, DB bottlenecks.
- [ ] Check: Horizontal scaling strategy c√≥ realistic kh√¥ng?
- C√≥ d·ª±a tr√™n per-instance benchmark? c√≥ headroom? c√≥ autoscaling triggers?
- [ ] Check: Load balancer configuration ƒë√∫ng ch∆∞a?
- L4/L7 ƒë√∫ng ch∆∞a, health-check ƒë√∫ng endpoint ch∆∞a, connection draining, timeouts.
- [ ] Check: Stateless design ƒë√∫ng ch∆∞a? (no session state?)
- Session/cached state c√≥ n·∫±m local kh√¥ng? c√≥ Redis/DB shared kh√¥ng?
- [ ] Check: SPOFs eliminated ch∆∞a?
- LB/DB/cache/queue c√≥ HA ch∆∞a, secrets/config c√≥ replicated ch∆∞a?
- [ ] Identify: 3 weaknesses trong design
- M·∫´u: External API dependency, DB single-writer bottleneck, thi·∫øu backpressure.
- [ ] Propose: Improvements cho weaknesses
- M·∫´u: cache+fallback, CQRS/read replicas, queue + rate limit + circuit breaker.
- [ ] Compare: Design vs industry best practices
- So v·ªõi: stateless app tier, managed LB, safe deploy, observability, DR.
- [ ] Document: Architecture review findings
- 1‚Äì2 trang: v·∫•n ƒë·ªÅ ‚Üí impact ‚Üí fix ‚Üí trade-off.

### Code Review

- [ ] Review: Stateless application code
- Check: kh√¥ng d√πng HttpSession/in-memory state quy·∫øt ƒë·ªãnh response.
- [ ] Check: No session state trong code?
- N·∫øu c√≥ login/session: ph·∫£i externalize (JWT/Redis), kh√¥ng pin v√†o instance.
- [ ] Check: JWT implementation correct?
- Verify: expiry, issuer/audience, signature alg, key rotation, refresh token, revoke strategy (n·∫øu c·∫ßn).
- [ ] Review: Load balancer implementation
- Check: thread-safety, health-check scheduling, remove/add backend ƒë√∫ng.
- [ ] Check: Algorithm implementation correct?
- Test cases: RR wrap-around, WRR weights, least-conn accuracy.
- [ ] Check: Health check logic correct?
- Active check + thresholds; tr√°nh flapping (rise/fall); timeout nh·ªè.
- [ ] Review: Circuit breaker implementation
- Check: failure threshold, half-open probes, metrics.
- [ ] Check: Configuration correct?
- Timeouts < CB window; retry kh√¥ng ‚Äúƒë√°nh‚Äù v√†o CB sai c√°ch.
- [ ] Check: Fallback mechanism works?
- Fallback kh√¥ng l√†m sai business (payment th∆∞·ªùng kh√¥ng ƒë∆∞·ª£c ‚Äúfake success‚Äù).
- [ ] Identify: 3 code improvements
- M·∫´u: better timeouts, structured logs + traceId, idempotency key handling.
- [ ] Refactor: At least 1 piece of code
- M·∫´u: t√°ch LB policy kh·ªèi health-check scheduler; th√™m interfaces/testability.
- [ ] Document: Code review findings
- Link commit + notes + test evidence.

### Performance Review

- [ ] Review: Load test results
- Nh√¨n P95/P99, saturation, error rate, throughput; ƒë·ª´ng ch·ªâ nh√¨n average.
- [ ] Review: Failure simulation results
- So s√°nh tr∆∞·ªõc/sau tuning health-check/timeout/retry.
- [ ] Measure: System performance under normal load
- Baseline: QPS at P95<SLO, CPU<70%, DB<80% capacity.
- [ ] Measure: System performance under failure scenarios
- Track: error spike window, recovery time, tail latency.
- [ ] Identify: Performance bottlenecks
- Th∆∞·ªùng g·∫∑p: DB locks, connection pool, GC, synchronous downstream calls.
- [ ] Verify: Scaling works as expected?
- Khi tƒÉng instances 2x, throughput c√≥ tƒÉng g·∫ßn 2x kh√¥ng? n·∫øu kh√¥ng, bottleneck ·ªü ƒë√¢u.
- [ ] Document: Performance analysis
- Ghi: graphs + root cause + actions.
- [ ] Create: Performance improvement plan
- 3 items: quick wins (timeouts/cache), mid (DB index/read replicas), long (sharding/async).

### Knowledge Check

- [ ] Explain: Horizontal vs Vertical scaling (vi·∫øt 1 paragraph, kh√¥ng xem notes)
- Sample (ng·∫Øn): Vertical = scale up 1 m√°y (CPU/RAM), nhanh nh∆∞ng gi·ªõi h·∫°n & SPOF; Horizontal = scale out nhi·ªÅu m√°y, c·∫ßn LB + statelessness, ph·ª©c t·∫°p h∆°n nh∆∞ng HA/scalability t·ªët.
- [ ] Explain: Load balancer algorithms (vi·∫øt comparison, kh√¥ng xem notes)
- Sample: RR (ƒë∆°n gi·∫£n, backend ƒë·ªìng nh·∫•t), WRR (backend kh√°c capacity), LeastConn (workload bi·∫øn thi√™n/connection d√†i), IP hash (sticky), LeastRT (t·ªëi ∆∞u latency nh∆∞ng c·∫ßn measurement).
- [ ] Explain: Stateless vs Stateful (vi·∫øt 1 paragraph, kh√¥ng xem notes)
- Sample: Stateless kh√¥ng gi·ªØ session local; state externalize ‚Üí d·ªÖ scale/HA. Stateful gi·ªØ state local ‚Üí c·∫ßn sticky/replication, deploy/failover kh√≥.
- [ ] Explain: Active-Active vs Active-Passive (vi·∫øt comparison, kh√¥ng xem notes)
- Sample: Active-Active ph·ª•c v·ª• song song ‚Üí failover nhanh nh∆∞ng ph·ª©c t·∫°p consistency; Active-Passive ƒë∆°n gi·∫£n h∆°n nh∆∞ng failover c√≥ RTO v√† l√£ng ph√≠.
- [ ] Explain: Zero-downtime deployment strategies (vi·∫øt 1 paragraph, kh√¥ng xem notes)
- Sample: Blue-green switch m√¥i tr∆∞·ªùng, rollback nhanh; Canary shift d·∫ßn traffic gi·∫£m risk; Rolling thay d·∫ßn instances y√™u c·∫ßu backward-compatible.
- [ ] Solve: System c√≥ 3 components (99%, 99.9%, 99.99%), 2 app servers parallel (99.9% each), t√≠nh overall availability
    - App parallel: \(A_{app}=1-(1-0.999)^2 = 0.999999\).
    - Overall series: \(A = 0.99 \times 0.9999 \times 0.9999 \times 0.999999 \approx 0.9897\) (**~98.97%**).
- [ ] Solve: Current 1K QPS, growth 30%/month, c·∫ßn capacity sau 6 th√°ng? (v·ªõi 20% headroom)
    - \(QPS_6 = 1000 \times 1.3^6 \approx 4827\) QPS.
    - Headroom 20%: \(4827 \times 1.2 \approx 5792\) QPS capacity.
- [ ] Verify: Answers c√≥ ƒë√∫ng kh√¥ng?
- Check l·∫°i c√¥ng th·ª©c series/parallel + l√†m tr√≤n; ghi gi·∫£ ƒë·ªãnh independence.
- [ ] Document: Knowledge gaps
- Ghi 5 bullet: m√¨nh ch∆∞a ch·∫Øc ch·ªó n√†o + plan h·ªçc b√π.

### Reflection

- [ ] Write: 3 ƒëi·ªÅu h·ªçc ƒë∆∞·ª£c quan tr·ªçng nh·∫•t tu·∫ßn n√†y
- M·∫´u: (1) LB l√† ‚Äúc·ªët l√µi‚Äù c·ªßa HA, (2) stateless + external state l√† ch√¨a kho√° scale, (3) timeout/retry/CB quy·∫øt ƒë·ªãnh stability.
- [ ] Write: 2 concepts c√≤n confuse
- M·∫´u: (1) DB failover tooling & split-brain, (2) safe DB migrations cho zero-downtime.
- [ ] Write: 1 mistake ƒë√£ l√†m v√† lesson learned
- M·∫´u: retry kh√¥ng jitter g√¢y spike; lesson: retry budget + jitter + CB.
- [ ] Write: Confidence level cho Week 3 (1-10)
- G·ª£i √Ω: 7 n·∫øu b·∫°n t·ª± thi·∫øt k·∫ø DB schema + index + replication basics.
- [ ] Compare: Week 1 vs Week 2 progress
- Week1 fundamentals; Week2 th·ª±c chi·∫øn HA/scale (LB + stateless + deploy).
- [ ] Plan: Preparation cho Week 3 (Database Design)
- Plan 5 ng√†y: modeling + indexing + transactions + replication + caching patterns.
- [ ] Set: Goals cho Week 3
- Goals: thi·∫øt k·∫ø schema + query patterns + scaling DB (read replicas/sharding intro).
- [ ] Document: Week 2 reflection (500 words)
- B·ªë c·ª•c: what learned ‚Üí what failed ‚Üí what next ‚Üí metrics (labs done).

### Mentor Questions (Answer these - be honest)

- [ ] Q1: B·∫°n c√≥ th·ªÉ scale m·ªôt stateful service horizontally kh√¥ng? T·∫°i sao? (vi·∫øt analysis)
- [ ] Q2: Load balancer v·ªõi sticky sessions - c√≥ ph·∫£i stateless kh√¥ng? Gi·∫£i th√≠ch. (vi·∫øt answer)
- [ ] Q3: Active-Active setup c√≥ eliminate SPOF kh√¥ng? T·∫°i sao? (vi·∫øt analysis)
- [ ] Q4: Zero-downtime deployment c√≥ nghƒ©a l√† zero risk kh√¥ng? Gi·∫£i th√≠ch. (vi·∫øt answer)
- [ ] Q5: N·∫øu b·∫°n c√≥ 99.9% availability nh∆∞ng v·∫´n b·ªã complain, c√≥ th·ªÉ do g√¨? (vi·∫øt analysis)
- [ ] Q6: Horizontal scaling c√≥ gi·ªõi h·∫°n kh√¥ng? Gi·ªõi h·∫°n l√† g√¨? (vi·∫øt answer)
- [ ] Review: Answers c√≥ ƒë·ªß depth v√† critical thinking ch∆∞a?
- [ ] Improve: Answers n·∫øu c·∫ßn

---

## Final Checklist

- [ ] T·∫•t c·∫£ Study TODOs: ‚úÖ Complete
- [ ] T·∫•t c·∫£ Design TODOs: ‚úÖ Complete v·ªõi diagrams
- [ ] T·∫•t c·∫£ Coding TODOs: ‚úÖ Complete, tested, v√† documented
- [ ] T·∫•t c·∫£ Failure & Resilience TODOs: ‚úÖ Complete v·ªõi results
- [ ] T·∫•t c·∫£ Analysis TODOs: ‚úÖ Complete v·ªõi documentation
- [ ] T·∫•t c·∫£ Review TODOs: ‚úÖ Complete
- [ ] Reflection document: ‚úÖ Written
- [ ] Code committed: ‚úÖ Yes
- [ ] Design diagrams saved: ‚úÖ Yes
- [ ] Ready for Week 3: ‚úÖ Yes

---

> **Mentor Final Check**: Tu·∫ßn 2 n√†y b·∫°n ph·∫£i MASTER scaling v√† high availability. N·∫øu b·∫°n ch·ªâ hi·ªÉu l√Ω thuy·∫øt m√† kh√¥ng
> implement ƒë∆∞·ª£c, b·∫°n ch∆∞a s·∫µn s√†ng. H√£y honest: B·∫°n c√≥ th·ªÉ design v√† implement m·ªôt h·ªá th·ªëng scalable, highly available
> ch∆∞a? N·∫øu ch∆∞a, l√†m l·∫°i.
